# -*- coding: utf-8 -*-
"""RGB_4.0ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oEHwbd6EZb64Pv40ufP0XxXIwO8Ac43V

```
Models tested
llama-3.3-70b-versatile
llama-3.1-8b-instant
gemma2-9b-it
qwen/qwen3-32b
deepseek-r1-distill-llama-70b

Files needed:

Datase files  - en, en_refine, en_fact, en_int
Instruction.yaml, instruction_fact.yaml



```
"""

import pandas as pd
import os
import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, GenerationConfig
import requests
import json
import time
import math
import yaml
import random
import tqdm
import numpy as np
import requests # Import requests for Groq API calls
from google.colab import userdata # Import userdata for API key
import re
import time

# ======================== SIMPLE CONFIG ========================
GENERATION_MODEL = "gemma2-9b-it"  # Groq model for generation
JUDGMENT_MODEL = "llama-3.3-70b-versatile" #"llama-3.3-70b-versatile"  # DeepSeek model through Groq for judgment
Temperature = 0.2

# ======================== HELPER FUNCTIONS ========================
def create_default_configs(use_custom_prompt=False):
    """
    Create configuration files with flexible prompt selection
    - Used by BOTH noise ratio AND negative rejection
    """

    if use_custom_prompt:
        print("Using CUSTOM OPTIMIZED prompt...")
        default_config = {
            'en': {
                'system': """You are an accurate and reliable AI assistant that can answer questions with the help of external documents.
Please note that external documents may contain noisy or factually incorrect information.
INSTRUCTIONS:
- If the information in the document contains the correct answer, you will give an accurate answer
- If the information in the document does not contain the answer, you will generate 'I can not answer the question because of the insufficient information in documents.'
- Be precise and only answer what can be directly supported by the documents
- Do not make assumptions beyond what is explicitly stated""",
                'instruction': 'Document:\n{DOCS}\n\nQuestion:\n{QUERY}'
            }
        }
        with open('/content/instruction.yaml', 'w') as f:
            yaml.dump(default_config, f)

    else:
        if os.path.exists('/content/instruction.yaml'):
            print("📋 Using EXISTING YAML file prompt...")
            return
        else:
            print("🔧 Creating BASIC DEFAULT prompt...")
            default_config = {
                'en': {
                    'system': 'You are a helpful assistant that answers questions based on provided documents.',
                    'instruction': 'Based on the following documents:\n{DOCS}\n\nQuestion: {QUERY}\nAnswer:'
                }
            }
            with open('/content/instruction.yaml', 'w') as f:
                yaml.dump(default_config, f)

    # Fact-checking config (if needed)
    if not os.path.exists('/content/instruction_fact.yaml'):
        fact_config = {
            'en': {
                'system': 'You are a helpful assistant that identifies factual errors in documents and answers questions.',
                'instruction': 'Based on the following documents:\n{DOCS}\n\nQuestion: {QUERY}\n\nPlease check for any factual errors in the documents and provide your answer. If you find factual errors, mention "factual errors" in your response.'
            }
        }
        with open('/content/instruction_fact.yaml', 'w') as f:
            yaml.dump(fact_config, f)

base_path = "/content/"

df_en = pd.read_json(os.path.join(base_path, "en.json"), lines=True)
df_fact = pd.read_json(os.path.join(base_path, "en_fact.json"), lines=True)
df_int = pd.read_json(os.path.join(base_path, "en_int.json"), lines=True)
df_refine = pd.read_json(os.path.join(base_path, "en_refine.json"), lines=True)
df_en.head()

# models_new.py

class MistralInstruct:
    def __init__(self, plm='mistralai/Mistral-7B-Instruct-v0.2') -> None:
        self.plm = plm
        self.tokenizer = AutoTokenizer.from_pretrained(plm)
        self.model = AutoModelForCausalLM.from_pretrained(plm, device_map="auto", torch_dtype=torch.float16).eval()

    def generate(self, text, temperature=0.2, system="", top_p=0.8, max_new_tokens=256):
        messages = []
        if len(system) > 0:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors="pt")
        model_inputs = encodeds.to(self.model.device)

        generated_ids = self.model.generate(model_inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p)
        decoded = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        # The output includes the prompt, so we need to remove it
        response = decoded[len(self.tokenizer.apply_chat_template(messages, tokenize=False)):].strip()
        return response

class GemmaInstruct:
    def __init__(self, plm='google/gemma-7b-instruct') -> None:
        self.plm = plm
        self.tokenizer = AutoTokenizer.from_pretrained(plm)
        self.model = AutoModelForCausalLM.from_pretrained(plm, device_map="auto", torch_dtype=torch.float16).eval()

    def generate(self, text, temperature=0.7, system="", top_p=0.8, max_new_tokens=256):
        messages = []
        if len(system) > 0:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        chat_template = "{% for message in messages %}{% if message['role'] == 'user' %}<start_of_turn>user\n{{ message['content'] }}<end_of_turn>\n{% elif message['role'] == 'system' %}<start_of_turn>system\n{{ message['content'] }}<end_of_turn>\n{% elif message['role'] == 'model' %}<start_of_turn>model\n{{ message['content'] }}<end_of_turn>\n{% endif %}{% if loop.last %}{% else %}\n{% endif %}{% endfor %}"
        self.tokenizer.apply_chat_template = chat_template

        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors="pt")
        model_inputs = encodeds.to(self.model.device)

        generated_ids = self.model.generate(model_inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p)
        decoded = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        # The output includes the prompt, so we need to remove it
        response = decoded[len(self.tokenizer.apply_chat_template(messages, tokenize=False)):].strip()
        return response

class Llama3Instruct:
    def __init__(self, plm='meta-llama/Meta-Llama-3-8B-Instruct') -> None:
        self.plm = plm
        self.tokenizer = AutoTokenizer.from_pretrained(plm)
        self.model = AutoModelForCausalLM.from_pretrained(plm, device_map="auto", torch_dtype=torch.float16).eval()

    def generate(self, text, temperature=0.7, system="", top_p=0.8, max_new_tokens=256):
        messages = []
        if len(system) > 0:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors="pt")
        model_inputs = encodeds.to(self.model.device)

        generated_ids = self.model.generate(model_inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p)
        decoded = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        # The output includes the prompt, so we need to remove it
        response = decoded[len(self.tokenizer.apply_chat_template(messages, tokenize=False)):].strip()
        return response

class OpenAIAPIModel:
    def __init__(self, api_key, url="https://api.openai.com/v1/chat/completions"):
        self.api_key = api_key
        self.url = url

    def generate(self, text, temperature=0.7, system="", top_p=1.0, max_new_tokens=512):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        data = {
            "model": "gpt-3.5-turbo", # Explicitly set the model to gpt-3.5-turbo
            "messages": messages,
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_new_tokens,
        }

        response = requests.post(self.url, headers=headers, json=data)
        response.raise_for_status() # Raise an exception for bad status codes
        return response.json()['choices'][0]['message']['content']


class Qwen3_32B:
    def __init__(self, plm='Qwen/Qwen3-32B') -> None:
        self.plm = plm
        self.tokenizer = AutoTokenizer.from_pretrained(plm, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(plm, device_map="auto", trust_remote_code=True).eval()

    def generate(self, text, temperature=0.8, system="", top_p=0.8):
        messages = []
        if len(system) > 0:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            temperature=temperature,
            top_p=top_p,
        )
        generated_ids = [
            output_ids[len(input_inputs):] for input_inputs, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response

class DeepseekR1DistillLlama70B:
    def __init__(self, plm='deepseek-ai/deepseek-r1-distill-llama-70b') -> None:
        self.plm = plm
        self.tokenizer = AutoTokenizer.from_pretrained(plm, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(plm, device_map="auto", trust_remote_code=True).eval()

    def generate(self, text, temperature=0.8, system="", top_p=0.8):
        messages = []
        if len(system) > 0:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            temperature=temperature,
            top_p=top_p,
        )
        generated_ids = [
            output_ids[len(input_inputs):] for input_inputs, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response

class Llama4Maverick17B128eInstruct:
    def __init__(self, plm='meta-llama/llama-4-maverick-17b-128e-instruct') -> None:
        self.plm = plm
        self.tokenizer = AutoTokenizer.from_pretrained(plm, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(plm, device_map="auto", trust_remote_code=True).eval()

    def generate(self, text, temperature=0.7, system="You are a helpful, respectful and honest assistant.", top_p=0.8, max_new_tokens=256):
        messages = []
        if len(system) > 0:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(
            model_inputs.input_ids,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        generated_ids = [
            output_ids[len(input_inputs):] for input_inputs, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response


class QwenQwq32B:
    def __init__(self, plm='qwen-qwq-32b') -> None:
        self.plm = plm
        self.tokenizer = AutoTokenizer.from_pretrained(plm, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(plm, device_map="auto", trust_remote_code=True).eval()

    def generate(self, text, temperature=0.8, system="", top_p=0.8):
        messages = []
        if len(system) > 0:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            temperature=temperature,
            top_p=top_p,
        )
        generated_ids = [
            output_ids[len(input_inputs):] for input_inputs, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response


class Llama3370BVersatile:
    def __init__(self, plm='llama-3.3-70b-versatile') -> None:
        self.plm = plm
        self.tokenizer = AutoTokenizer.from_pretrained(plm, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(plm, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True).eval()

    def get_prompt(self, message: str, chat_history: list[tuple[str, str]],
               system_prompt: str) -> str:
        texts = [f'<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n']
        # The first user input is _not_ stripped
        do_strip = False
        for user_input, response in chat_history:
            user_input = user_input.strip() if do_strip else user_input
            do_strip = True
            texts.append(f'{user_input} [/INST] {response.strip()} </s><s>[INST] ')
        message = message.strip() if do_strip else message
        texts.append(f'{message} [/INST]')
        return ''.join(texts)

    def generate(self, text, temperature=0.7, system="You are a helpful, respectful and honest assistant.", top_p=0.8, max_new_tokens=256):
        query = self.get_prompt(text, [], system)

        inputs = self.tokenizer(query, return_tensors="pt", add_special_tokens=False,return_token_type_ids=False)
        for k in inputs:
            inputs[k] = inputs[k].cuda()

        outputs = self.model.generate(**inputs, do_sample=True, temperature=temperature, top_p=top_p, max_length=max_new_tokens + inputs['input_ids'].size(-1))
        response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        return response


class Llama4Scout17B16eInstruct:
    def __init__(self, plm='meta-llama/llama-4-scout-17b-16e-instruct') -> None:
        self.plm = plm
        self.tokenizer = AutoTokenizer.from_pretrained(plm, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(plm, device_map="auto", trust_remote_code=True).eval()

    def generate(self, text, temperature=0.7, system="You are a helpful, respectful and honest assistant.", top_p=0.8, max_new_tokens=256):
        messages = []
        if len(system) > 0:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(
            model_inputs.input_ids,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        generated_ids = [
            output_ids[len(input_inputs):] for input_inputs, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response

class GroqModel:
    def __init__(self, api_key, plm='llama3-8b-8192', sleep_seconds=2.0, max_retries=5):
        self.api_key = api_key
        self.plm = plm
        self.url = "https://api.groq.com/openai/v1/chat/completions"
        self.sleep_seconds = sleep_seconds
        self.max_retries = max_retries
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    def generate(self, text, temperature=0.7, system="", top_p=0.8,
                 max_new_tokens=512):

        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        data = {
            "model": self.plm,
            "messages": messages,
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_new_tokens
        }

        # Retry with sleep
        for attempt in range(1, self.max_retries + 1):
            try:
                response = requests.post(self.url, headers=self.headers, json=data, timeout=15)
                response.raise_for_status()
                # ✅ sleep between successful calls
                time.sleep(self.sleep_seconds)
                return response.json()['choices'][0]['message']['content']
            except requests.exceptions.RequestException as e:
                print(f"[Retry {attempt}/{self.max_retries}] Groq API error: {e}")
                time.sleep(2 * attempt)  # Exponential backoff
        raise RuntimeError("Groq API failed after maximum retries.")


class GroqModel_old:
    def __init__(self, api_key, plm='llama3-8b-8192'):
        self.api_key = api_key
        self.plm = plm
        self.url = "https://api.groq.com/openai/v1/chat/completions"

    def generate(self, text, temperature=0.7, system="", top_p=0.8, max_new_tokens=3500):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": text})

        data = {
            "model": self.plm,
            "messages": messages,
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_new_tokens,
        }

        response = requests.post(self.url, headers=headers, json=data)
        response.raise_for_status() # Raise an exception for bad status codes
        return response.json()['choices'][0]['message']['content']

def processdata(instance, noise_rate, passage_num, filename, correct_rate=0):
    query = instance['query']
    ans = instance['answer']
    neg_num = math.ceil(passage_num * noise_rate)
    pos_num = passage_num - neg_num

    if '_int' in filename:
        for i in instance['positive']:
            random.shuffle(i)
        docs = [i[0] for i in instance['positive']]
        if len(docs) < pos_num:
            maxnum = max([len(i) for i in instance['positive']])
            for i in range(1, maxnum):
                for j in instance['positive']:
                    if len(j) > i:
                        docs.append(j[i])
                        if len(docs) == pos_num:
                            break
                if len(docs) == pos_num:
                    break
        neg_num = passage_num - len(docs)
        if neg_num > 0:
            docs += instance['negative'][:neg_num]

    elif '_fact' in filename:
        correct_num = math.ceil(passage_num * correct_rate)
        pos_num = passage_num - neg_num - correct_num
        indexs = list(range(len(instance['positive'])))
        selected = random.sample(indexs, min(len(indexs), pos_num))
        docs = [instance['positive_wrong'][i] for i in selected]
        remain = [i for i in indexs if i not in selected]
        if correct_num > 0 and len(remain) > 0:
            docs += [instance['positive'][i] for i in random.sample(remain, min(len(remain), correct_num))]
        if neg_num > 0:
            docs += instance['negative'][:neg_num]
    else:
        positive = instance['positive'][:pos_num]
        negative = instance['negative'][:neg_num]
        docs = positive + negative

    random.shuffle(docs)
    return query, ans, docs

def checkanswer(prediction, ground_truth):
    prediction = prediction.lower()
    if not isinstance(ground_truth, list):
        ground_truth = [ground_truth]

    labels = []
    for instance in ground_truth:
        flag = True
        if isinstance(instance, list):
            flag = False
            for i in instance:
                if isinstance(i, str) and i.lower() in prediction:
                    flag = True
                    break
        elif isinstance(instance, str):
            if instance.lower() not in prediction:
                flag = False
        else:
            # Skip if instance is neither string nor list
            flag = False
        labels.append(int(flag))
    return labels


def predict(query, ground_truth, docs, model, system, instruction, temperature, dataset):
    prediction = ""
    try:
        if len(docs) == 0:
            text = instruction.format(QUERY=query, DOCS='')
        else:
            docs_text = '\n'.join(docs)
            text = instruction.format(QUERY=query, DOCS=docs_text)

        prediction = model.generate(text=text, temperature=temperature,system=system,max_new_tokens=512)
    except Exception as e:
        print(f"❌ Error during model generation for query: {query[:50]}... Error: {e}") # More specific error message
        prediction = "" # Set prediction to empty string if generation fails


    if 'zh' in dataset:
        prediction = prediction.replace(" ", "")

    if '信息不足' in prediction or 'insufficient information' in prediction:
        labels = [-1]
    else:
        labels = checkanswer(prediction, ground_truth)

    factlabel = int('事实性错误' in prediction or 'factual errors' in prediction)
    return labels, prediction, factlabel

def run_rgb_eval_new(
    modelname="chatglm3",
    dataset="en",
    api_key=None,
    plm="THUDM/chatglm3-6b",
    url="https://api.openai.com/v1/chat/completions",
    temperature=Temperature,
    noise_rate=0.0,
    correct_rate=0.0,
    passage_num=5,
    factchecking=False,
    use_custom_prompt=False  # 🆕 NEW PARAMETER
):
    # 🆕 ADD THIS LINE - Use flexible prompt configuration
    #create_default_configs(use_custom_prompt=use_custom_prompt)

    with open(f'/content/{dataset}.json','r') as f:
        instances = [json.loads(line) for line in f]

    resultpath = f"result-{dataset[:2]}"
    if factchecking:
        prompt = yaml.safe_load(open('/content/instruction_fact.yaml'))[dataset[:2]]
        resultpath += "/fact"
        os.makedirs(f"/content/{resultpath}", exist_ok=True)
        filename=f"/content/{resultpath}/prediction_{dataset}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}.json"
    else:
        prompt = yaml.safe_load(open('/content/instruction.yaml'))[dataset[:2]]
        filename=f"/content/{resultpath}/prediction_{dataset}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}.json"

    os.makedirs(resultpath, exist_ok=True)

    system = prompt['system']
    instruction = prompt['instruction']

    # Use the new model classes based on modelname
    if modelname == 'Qwen3_32B':
        model = Qwen3_32B(plm)
    elif modelname == 'DeepseekR1DistillLlama70B':
        model = DeepseekR1DistillLlama70B(plm)
    elif modelname == 'Llama4Maverick17B128eInstruct':
        model = Llama4Maverick17B128eInstruct(plm)
    elif modelname == 'QwenQwq32B':
        model = QwenQwq32B(plm)
    elif modelname == 'Llama3370BVersatile':
        model = Llama3370BVersatile(plm)
    elif modelname == 'Llama4Scout17B16eInstruct':
        model = Llama4Scout17B16eInstruct(plm)
    elif modelname == 'Groq':
        api_key = userdata.get('GROQ_API_KEY') # Get Groq API key from secrets
        model = GroqModel(api_key=api_key, plm=plm) # Use the provided plm for Groq
    elif modelname == 'chatgpt':
        model = OpenAIAPIModel(api_key=api_key, url=url)
    elif modelname == 'MistralInstruct':
        model = MistralInstruct(plm)
    elif modelname == 'GemmaInstruct':
        model = GemmaInstruct(plm)
    elif modelname == 'Llama3Instruct':
        model = Llama3Instruct(plm)

    filename = f"/content/{resultpath}/prediction_{dataset}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}.json"
    useddata = {}
    if os.path.exists(filename):
        with open(filename) as f:
            for line in f:
                d = json.loads(line)
                useddata[d["id"]] = d

    results = []
    with open(filename, 'w') as f:
        for instance in tqdm.tqdm(instances):
            if instance['id'] in useddata and instance['query'] == useddata[instance['id']]['query']:
                results.append(useddata[instance['id']])
                f.write(json.dumps(useddata[instance['id']], ensure_ascii=False)+'\n')
                continue
            try:
                random.seed(2333)
                if passage_num == 0:
                    query = instance['query']
                    ans = instance['answer']
                    docs = []
                else:
                    query, ans, docs = processdata(instance, noise_rate, passage_num, dataset, correct_rate)

                # Handle potential errors during prediction and continue
                label, prediction, factlabel = predict(query, ans, docs, model, system, instruction, temperature, dataset)

                new = {
                    "id": instance["id"],
                    "query": query,
                    "ans": ans,
                    "label": label,
                    "prediction": prediction,
                    "docs": docs,
                    "noise_rate": noise_rate,
                    "factlabel": factlabel
                }
                results.append(new)
                f.write(json.dumps(new, ensure_ascii=False)+'\n')
            except Exception as e:
                print(f"❌ Error processing instance ID {instance['id']}: {e}") # More specific error message
                continue # Continue to the next instance even if one fails

    print(f"Length of results list: {len(results)}") # Add print statement
    tt = sum(1 for r in results if (noise_rate == 1 and r["label"][0] == -1) or (0 not in r["label"] and 1 in r["label"]))
    scores = {
        "all_rate": tt / len(results) if len(results) > 0 else 0, # Add check for division by zero
        "noise_rate": noise_rate,
        "tt": tt,
        "nums": len(results)
    }

    if "_fact" in dataset:
        fact_tt = sum(1 for r in results if r["factlabel"] == 1)
        correct_tt = sum(1 for r in results if r["factlabel"] == 1 and 0 not in r["label"])
        scores.update({
            "fact_check_rate": fact_tt / len(results) if len(results) > 0 else 0, # Add check for division by zero
            "correct_rate": correct_tt / fact_tt if fact_tt > 0 else 0,
            "fact_tt": fact_tt,
            "correct_tt": correct_tt
        })

    json.dump(scores, open(filename.replace(".json", "_result.json"), 'w'), indent=4, ensure_ascii=False)
    print("\n✅ Done. Score Summary:\n", json.dumps(scores, indent=2))

def export_noise_ratio_to_csv(model_name, temperature_list, noise_rate_list, dataset="en"):
    """
    Export all noise ratio results to comprehensive CSV files
    Works with existing run_rgb_eval_new output format
    """
    import pandas as pd

    print("📊 EXPORTING NOISE RATIO RESULTS TO CSV...")
    print(f"Model: {model_name}")
    print(f"Temperatures: {temperature_list}")
    print(f"Noise Ratios: {noise_rate_list}")
    print("-" * 50)

    all_detailed_data = []
    all_summary_data = []

    resultpath = f"result-{dataset[:2]}"

    for temp in temperature_list:
        for noise in noise_rate_list:
            # Use the exact filename pattern from run_rgb_eval_new
            prediction_file = f"/content/{resultpath}/prediction_{dataset}_Groq_temp{temp}_noise{noise}_passage5_correct0.0.json"
            result_file = f"/content/{resultpath}/prediction_{dataset}_Groq_temp{temp}_noise{noise}_passage5_correct0.0_result.json"

            print(f"Processing: temp={temp}, noise={noise}")

            # Load prediction results (your existing format)
            if os.path.exists(prediction_file):
                with open(prediction_file, 'r') as f:
                    predictions = [json.loads(line) for line in f]

                # Load summary scores (your existing format)
                summary_scores = {}
                if os.path.exists(result_file):
                    with open(result_file, 'r') as f:
                        summary_scores = json.load(f)

                # Process each prediction using your data structure
                for i, pred in enumerate(predictions):
                    # Use your label structure: [-1] for rejection, [0] for wrong, [1] for correct
                    label = pred.get('label', [])

                    # Determine answer status using your logic
                    is_rejection = (label == [-1])
                    answer_correct = (0 not in label and 1 in label) if not is_rejection else False

                    # Analyze prediction content
                    prediction_text = pred.get('prediction', '')
                    has_insufficient_info = 'insufficient information' in prediction_text.lower()
                    has_cannot_answer = 'cannot answer' in prediction_text.lower() or "can't answer" in prediction_text.lower()
                    has_factual_errors = 'factual errors' in prediction_text.lower() or '事实性错误' in prediction_text

                    # Determine response type using your classification
                    if is_rejection:
                        response_type = "REJECTION"
                    elif answer_correct:
                        response_type = "CORRECT_ANSWER"
                    else:
                        response_type = "INCORRECT_ANSWER"

                    # Count positive and negative documents (using your processdata logic)
                    docs = pred.get('docs', [])
                    passage_num = len(docs)
                    neg_num = math.ceil(passage_num * noise) if passage_num > 0 else 0
                    pos_num = passage_num - neg_num

                    detailed_row = {
                        "sample_id": i + 1,
                        "instance_id": pred.get("id", ""),
                        "model_name": model_name,
                        "temperature": temp,
                        "noise_ratio": noise,
                        "calculated_neg_docs": neg_num,
                        "calculated_pos_docs": pos_num,
                        "query": pred.get("query", ""),
                        "ground_truth": str(pred.get("ans", "")),
                        "prediction": prediction_text,
                        "prediction_length": len(prediction_text),
                        "label_raw": str(label),
                        "answer_correct": answer_correct,
                        "is_rejection": is_rejection,
                        "response_type": response_type,
                        "has_insufficient_info_phrase": has_insufficient_info,
                        "has_cannot_answer_phrase": has_cannot_answer,
                        "has_factual_errors_phrase": has_factual_errors,
                        "factlabel": pred.get("factlabel", 0),
                        "num_docs": len(docs),
                        "docs_preview": str(docs[:2])[:200] + "..." if docs else "",
                        "noise_rate_actual": pred.get("noise_rate", noise),
                        "experiment_type": "noise_ratio"
                    }
                    all_detailed_data.append(detailed_row)

                # Add summary data using your scores structure
                summary_row = {
                    "model_name": model_name,
                    "temperature": temp,
                    "noise_ratio": noise,
                    "total_samples": summary_scores.get("nums", len(predictions)),
                    "accuracy_rate": summary_scores.get("all_rate", 0) * 100,  # Your all_rate is 0-1
                    "correct_answers": summary_scores.get("tt", 0),  # Your tt count
                    "accuracy_rate_decimal": summary_scores.get("all_rate", 0),  # Keep original
                    "experiment_type": "noise_ratio"
                }
                all_summary_data.append(summary_row)

                print(f"  ✅ Processed {len(predictions)} samples - Accuracy: {summary_scores.get('all_rate', 0)*100:.1f}%")
            else:
                print(f"  ❌ File not found: {prediction_file}")

    # Create DataFrames
    df_detailed = pd.DataFrame(all_detailed_data)
    df_summary = pd.DataFrame(all_summary_data)

    # Generate filenames with model name
    model_clean = model_name.replace("/", "_").replace("-", "_")
    detailed_file = f'/content/noise_ratio_detailed_{model_clean}_results.csv'
    summary_file = f'/content/noise_ratio_summary_{model_clean}_results.csv'

    # Export to CSV
    if not df_detailed.empty:
        df_detailed.to_csv(detailed_file, index=False, encoding='utf-8')
        print(f"\n📁 Detailed results exported to: {detailed_file}")
        print(f"   Total rows: {len(df_detailed)}")

        # Show breakdown using your data structure
        print(f"\n📊 BREAKDOWN BY NOISE RATIO:")
        for noise in noise_rate_list:
            noise_data = df_detailed[df_detailed['noise_ratio'] == noise]
            if not noise_data.empty:
                correct_count = len(noise_data[noise_data['answer_correct'] == True])
                rejection_count = len(noise_data[noise_data['is_rejection'] == True])
                incorrect_count = len(noise_data[noise_data['response_type'] == 'INCORRECT_ANSWER'])
                total_count = len(noise_data)
                accuracy = (correct_count / total_count * 100) if total_count > 0 else 0
                print(f"  Noise {noise}: {accuracy:.1f}% accuracy ({correct_count}/{total_count})")
                print(f"    Correct: {correct_count}, Rejections: {rejection_count}, Incorrect: {incorrect_count}")

    if not df_summary.empty:
        df_summary.to_csv(summary_file, index=False, encoding='utf-8')
        print(f"\n📁 Summary results exported to: {summary_file}")

    # Create comprehensive analysis file
    if not df_detailed.empty:
        analysis_file = f'/content/noise_ratio_analysis_{model_clean}.csv'

        # Calculate RGB-style metrics
        analysis_data = []
        for noise in noise_rate_list:
            for temp in temperature_list:
                subset = df_detailed[(df_detailed['noise_ratio'] == noise) & (df_detailed['temperature'] == temp)]
                if not subset.empty:
                    total = len(subset)
                    correct = len(subset[subset['answer_correct'] == True])
                    rejections = len(subset[subset['is_rejection'] == True])
                    incorrect = total - correct - rejections

                    analysis_row = {
                        "model_name": model_name,
                        "temperature": temp,
                        "noise_ratio": noise,
                        "total_samples": total,
                        "correct_answers": correct,
                        "rejections": rejections,
                        "incorrect_answers": incorrect,
                        "accuracy_percent": (correct / total * 100) if total > 0 else 0,
                        "rejection_percent": (rejections / total * 100) if total > 0 else 0,
                        "error_percent": (incorrect / total * 100) if total > 0 else 0,
                        "avg_prediction_length": subset['prediction_length'].mean(),
                        "insufficient_info_phrases": len(subset[subset['has_insufficient_info_phrase'] == True]),
                        "cannot_answer_phrases": len(subset[subset['has_cannot_answer_phrase'] == True]),
                        "factual_error_phrases": len(subset[subset['has_factual_errors_phrase'] == True]),
                        "avg_docs_per_question": subset['num_docs'].mean()
                    }
                    analysis_data.append(analysis_row)

        df_analysis = pd.DataFrame(analysis_data)
        df_analysis.to_csv(analysis_file, index=False, encoding='utf-8')
        print(f"📁 Analysis results exported to: {analysis_file}")

    return df_detailed, df_summary

"""Single Usage"""

run_rgb_eval_new(
                modelname="Groq",
                dataset="en_refine",
                plm=GENERATION_MODEL,
                temperature=0.2,
                noise_rate=0.0,
                correct_rate=0.0,
                passage_num=5,
                factchecking=False
)

# Get all noise reobustness using various  noise rates
temperature_list = [0.2,0.5,0.7]
noise_rate_list = [0.0,0.2,0.4,0.2,0.4,0.8]

for temp in temperature_list:
    for noise in noise_rate_list:
        print(f"\n🔁 Running eval with temperature={temp}, noise_rate={noise}")
        try:
            run_rgb_eval_new(
                modelname="Groq",
                dataset="en_refine",
                plm=GENERATION_MODEL,
                temperature=temp,
                noise_rate=noise,
                correct_rate=0.0,
                passage_num=5,
                factchecking=False,
                use_custom_prompt=False  # Add this parameter
            )
        except Exception as e:
            print(f"❌ Failed for temp={temp}, noise={noise} — Error: {e}")

# Then export to CSV:
print(f"\n📊 EXPORTING ALL RESULTS TO CSV...")
df_detailed, df_summary = export_noise_ratio_to_csv(
    model_name=GENERATION_MODEL,
    temperature_list=temperature_list,
    noise_rate_list=noise_rate_list,
    dataset="en_refine"
)

"""**Negative Rejection**"""

def create_rgb_rejection_configs():
    """Create RGB paper instruction format for negative rejection"""
    if not os.path.exists('/content/instruction.yaml'):
        # RGB paper exact instruction format
        rejection_config = {
            'en': {
                'system': 'You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Please note that external documents may contain noisy or factually incorrect information. If the information in the document contains the correct answer, you will give an accurate answer. If the information in the document does not contain the answer, you will generate "I can not answer the question because of the insufficient information in documents." If there are inconsistencies with the facts in some of the documents, please generate the response "There are factual errors in the provided documents." and provide the correct answer.',
                'instruction': 'Document:\n{DOCS}\n\nQuestion:\n{QUERY}'
            }
        }
        with open('/content/instruction.yaml', 'w') as f:
            yaml.dump(rejection_config, f)

def processdata_negative_rejection(instance, passage_num=5):
    """
    RGB methodology for negative rejection:
    - Use only negative documents (all irrelevant to the question)
    - Shuffle them randomly
    """
    query = instance['query']
    ans = instance['answer']

    # Take only negative documents (irrelevant ones)
    docs = instance['negative'][:passage_num]
    random.shuffle(docs)
    return query, ans, docs

def predict_rejection(query, ground_truth, docs, model, system, instruction, temperature):
    """Generate prediction for negative rejection test"""
    prediction = ""
    try:
        docs_text = '\n'.join(docs) if docs else ''
        text = instruction.format(QUERY=query, DOCS=docs_text)
        prediction = model.generate(text=text, temperature=temperature, system=system, max_new_tokens=512)
    except Exception as e:
        print(f"Error during model generation: {str(e)}")
        prediction = ""

    return prediction

def check_exact_rejection(prediction):
    """More flexible rejection detection"""

    # Normalize the text
    pred_clean = prediction.lower().strip()

    # Core rejection patterns (more flexible)
    rejection_patterns = [
        r"i cannot answer.*insufficient.*information",
        r"i can\s*not answer.*insufficient.*information",
        r"insufficient information.*documents",
        r"cannot.*answer.*insufficient.*documents",
        r"not enough information.*documents",
        r"information.*not.*provided.*documents"
    ]

    import re
    return any(re.search(pattern, pred_clean) for pattern in rejection_patterns)

# def check_exact_rejection(prediction):
#     """
#     RGB Rej metric: Check for exact rejection phrase
#     """
#     # RGB paper exact phrase
#     exact_phrase = "I cannot answer the question because of the insufficient information in documents"
#     return exact_phrase.lower() in prediction.lower()

def evaluate_rejection_with_llm(query, prediction, api_key, judgment_model=JUDGMENT_MODEL):
    """
    RGB Rej* metric: Use LLM to evaluate rejection (ROBUST VERSION)
    """
    # Clearer, more direct prompt
#     judge_prompt = f"""Evaluate if the AI properly rejected answering due to insufficient information.

# AI was asked: {query}
# AI responded: {prediction}

# Did the AI indicate it cannot answer due to insufficient/irrelevant information? Answer ONLY with YES or NO."""
    judge_prompt = f"""You are an evaluator helping to determine if a language model correctly indicated that it could not answer a question based on provided documents.

The user provided a question and a response from another language model (the "generation model"). The generation model was instructed to say "I can not answer the question because of the insufficient information in documents." if it could not find the answer in the documents.

Based *only* on the generation model's response, determine if it indicated that it could not answer the question due to insufficient information in the documents.

Respond with ONLY "YES" if the response indicates inability to answer based on the documents, and ONLY "NO" otherwise. Do not provide any other text or explanation.

Question: {query}

Generation Model Response: {prediction}

Does the Generation Model Response indicate inability to answer based on the documents? (YES/NO)
"""
    try:
        judge_model = GroqModel(api_key=api_key, plm=judgment_model, sleep_seconds=1.0)
        evaluation = judge_model.generate(text=judge_prompt, temperature=0.0, max_new_tokens=10)

        # MUCH more flexible response checking
        eval_text = evaluation.strip().upper()

        # Check for various positive indicators
        positive_indicators = [
            "YES", "Y", "CORRECT", "PROPER", "APPROPRIATE",
            "DID INDICATE", "REJECTION", "INSUFFICIENT",
            "CANNOT ANSWER", "CAN'T ANSWER", "UNABLE"
        ]

        # Check for negative indicators
        negative_indicators = [
            "NO", "N", "INCORRECT", "IMPROPER", "INAPPROPRIATE",
            "DID NOT", "DIDN'T", "ATTEMPTED TO ANSWER", "PROVIDED AN ANSWER"
        ]

        # Score based on indicators
        positive_score = sum(1 for indicator in positive_indicators if indicator in eval_text)
        negative_score = sum(1 for indicator in negative_indicators if indicator in eval_text)

        # Decision logic
        if positive_score > negative_score:
            result = True
        elif negative_score > positive_score:
            result = False
        else:
            # Fallback: check if it starts with Y
            result = eval_text.startswith("Y")

        return result

    except Exception as e:
        print(f"❌ Judge error: {str(e)}")
        return False

def run_rgb_complete_negative_rejection(
    dataset="en_refine",
    modelname="Groq",
    temperature=0.0,
    passage_num=5,
    num_samples=300
):
    """
    Complete RGB Paper Implementation using your defined models
    """
    print("❌ RGB COMPLETE NEGATIVE REJECTION EVALUATION")
    print(f"Generation Model: {GENERATION_MODEL}")
    print(f"Judgment Model: {JUDGMENT_MODEL}")
    print(f"Evaluating ALL {num_samples} samples for both Rej and Rej*")
    print("-" * 70)

    create_rgb_rejection_configs()

    # Load dataset
    dataset_file = f'/content/{dataset}.json'
    with open(dataset_file,'r') as f:
        instances = [json.loads(line) for line in f]

    # RGB instruction format
    with open('/content/instruction.yaml', 'r') as f:
        prompt = yaml.safe_load(f)['en']
    system = prompt['system']
    instruction = prompt['instruction']

    # Initialize models using your defined models
    api_key = userdata.get('GROQ_API_KEY')
    generation_model = GroqModel(api_key=api_key, plm=GENERATION_MODEL)  # Your generation model

    # Output setup
    resultpath = f"result-en"
    os.makedirs(f"/content/{resultpath}", exist_ok=True)
    filename = f"/content/{resultpath}/complete_rejection_{modelname}_temp{temperature}.json"

    results = []
    exact_rejections = 0  # Rej metric
    llm_rejections = 0    # Rej* metric

    test_instances = instances[:min(num_samples, len(instances))]
    print(f"Processing {len(test_instances)} samples...")

    with open(filename, 'w') as f:
        for i, instance in enumerate(tqdm.tqdm(test_instances, desc="Complete RGB Evaluation")):
            try:
                random.seed(2333)
                query, ans, docs = processdata_negative_rejection(instance, passage_num)

                # Generate prediction
                prediction = predict_rejection(query, ans, docs, generation_model, system, instruction, temperature)

                # Rej: Exact phrase matching
                exact_rejection = check_exact_rejection(prediction)
                if exact_rejection:
                    exact_rejections += 1

                # Rej*: LLM evaluation using your JUDGMENT_MODEL
                llm_rejection = evaluate_rejection_with_llm(query, prediction, api_key, JUDGMENT_MODEL)
                if llm_rejection:
                    llm_rejections += 1

                # Additional analysis fields
                prediction_lower = prediction.lower()
                has_insufficient_info = "insufficient information" in prediction_lower
                has_cannot_answer = "cannot answer" in prediction_lower or "can't answer" in prediction_lower
                has_not_provided = "not provided" in prediction_lower
                has_unclear = "unclear" in prediction_lower

                # Determine status
                if exact_rejection:
                    status = "EXACT_REJECTION"
                elif llm_rejection:
                    status = "LLM_REJECTION"
                else:
                    status = "HALLUCINATION"

                result = {
                    "id": instance["id"],
                    "query": query,
                    "ground_truth": ans,
                    "prediction": prediction,
                    "docs": docs,
                    "exact_rejection": exact_rejection,
                    "llm_rejection": llm_rejection,
                    "status": status,
                    "has_insufficient_info": has_insufficient_info,
                    "has_cannot_answer": has_cannot_answer,
                    "has_not_provided": has_not_provided,
                    "has_unclear": has_unclear,
                    "prediction_length": len(prediction),
                    "num_docs": len(docs),
                    "generation_model": GENERATION_MODEL,  # Your model
                    "judgment_model": JUDGMENT_MODEL,      # Your model
                    "temperature": temperature
                }
                results.append(result)
                f.write(json.dumps(result, ensure_ascii=False)+'\n')

                # Progress update every 50 samples
                if (i + 1) % 50 == 0:
                    current_rej = (exact_rejections / (i + 1)) * 100
                    current_rej_star = (llm_rejections / (i + 1)) * 100
                    print(f"\nProgress: {i + 1}/{len(test_instances)} | Rej: {current_rej:.1f}% | Rej*: {current_rej_star:.1f}%")

            except Exception as e:
                print(f"Error processing instance {instance.get('id', 'unknown')}: {e}")
                continue

    # Calculate final metrics
    total = len(results)
    rej_rate = (exact_rejections / total * 100) if total > 0 else 0
    rej_star_rate = (llm_rejections / total * 100) if total > 0 else 0
    hallucination_rate = 100 - rej_rate

    scores = {
        "model": modelname,
        "generation_plm": GENERATION_MODEL,    # Your model
        "judgment_plm": JUDGMENT_MODEL,        # Your model
        "dataset": dataset,
        "temperature": temperature,
        "passage_num": passage_num,
        "total_instances": total,
        "exact_rejections": exact_rejections,
        "rej_rate": rej_rate,
        "llm_rejections": llm_rejections,
        "rej_star_rate": rej_star_rate,
        "hallucination_rate": hallucination_rate,
        "agreement_count": sum(1 for r in results if r['exact_rejection'] == r['llm_rejection']),
        "agreement_rate": (sum(1 for r in results if r['exact_rejection'] == r['llm_rejection']) / total * 100) if total > 0 else 0
    }

    # Save scores
    result_file = filename.replace(".json", "_scores.json")
    json.dump(scores, open(result_file, 'w'), indent=4, ensure_ascii=False)

    print(f"\n📊 COMPLETE RGB RESULTS:")
    print("-" * 50)
    print(f"Total Samples: {total}")
    print(f"Rej (Exact): {exact_rejections} ({rej_rate:.2f}%)")
    print(f"Rej* (LLM): {llm_rejections} ({rej_star_rate:.2f}%)")
    print(f"Hallucination Rate: {hallucination_rate:.2f}%")
    print(f"Rej vs Rej* Agreement: {scores['agreement_rate']:.2f}%")
    print("-" * 50)

    return scores, results

def export_complete_analysis_csv(results, scores):
    """Export complete analysis CSV with all samples and detailed metrics"""
    import pandas as pd

    print("📊 Exporting complete analysis CSV...")

    # Main detailed results
    csv_data = []
    for i, result in enumerate(results):
        csv_data.append({
            "sample_id": i + 1,
            "instance_id": result["id"],
            "query": result["query"],
            "ground_truth": result["ground_truth"],
            "prediction": result["prediction"],
            "prediction_length": result["prediction_length"],
            "exact_rejection_rej": result["exact_rejection"],
            "llm_rejection_rej_star": result["llm_rejection"],
            "rej_rej_star_agreement": result["exact_rejection"] == result["llm_rejection"],
            "status": result["status"],
            "has_insufficient_info_phrase": result["has_insufficient_info"],
            "has_cannot_answer_phrase": result["has_cannot_answer"],
            "has_not_provided_phrase": result["has_not_provided"],
            "has_unclear_phrase": result["has_unclear"],
            "num_irrelevant_docs": result["num_docs"],
            "docs_preview": str(result["docs"][:1])[1:100] + "..." if result["docs"] else "",
            "generation_model": result["generation_model"],
            "judgment_model": result["judgment_model"],
            "temperature": result["temperature"]
        })

    # Create main DataFrame
    df_main = pd.DataFrame(csv_data)

    # Export main results
    main_file = f'/content/rgb_complete_rejection_analysis.csv'
    df_main.to_csv(main_file, index=False, encoding='utf-8')
    print(f"📁 Complete analysis exported to: {main_file}")

    # Create summary statistics
    summary_stats = {
        "metric": [
            "Total Samples",
            "Exact Rejections (Rej)",
            "Rej Rate (%)",
            "LLM Rejections (Rej*)",
            "Rej* Rate (%)",
            "Hallucination Rate (%)",
            "Rej vs Rej* Agreement (%)",
            "Samples with 'insufficient info'",
            "Samples with 'cannot answer'",
            "Samples with 'not provided'",
            "Samples with 'unclear'",
            "Average Prediction Length",
            "Generation Model",
            "Judgment Model",
            "Temperature"
        ],
        "value": [
            scores["total_instances"],
            scores["exact_rejections"],
            f"{scores['rej_rate']:.2f}",
            scores["llm_rejections"],
            f"{scores['rej_star_rate']:.2f}",
            f"{scores['hallucination_rate']:.2f}",
            f"{scores['agreement_rate']:.2f}",
            sum(1 for r in results if r['has_insufficient_info']),
            sum(1 for r in results if r['has_cannot_answer']),
            sum(1 for r in results if r['has_not_provided']),
            sum(1 for r in results if r['has_unclear']),
            f"{sum(r['prediction_length'] for r in results) / len(results):.1f}",
            scores["generation_plm"],
            scores["judgment_plm"],
            scores["temperature"]
        ]
    }

    df_summary = pd.DataFrame(summary_stats)
    summary_file = f'/content/rgb_rejection_summary.csv'
    df_summary.to_csv(summary_file, index=False, encoding='utf-8')
    print(f"📁 Summary statistics exported to: {summary_file}")

    return df_main, df_summary

def run_complete_rgb_analysis():
    """Run complete RGB negative rejection analysis using your defined models"""
    print("🚀 COMPLETE RGB NEGATIVE REJECTION ANALYSIS")
    print(f"Using your models: {GENERATION_MODEL} → {JUDGMENT_MODEL}")
    print("=" * 70)

    try:
        # Run complete evaluation
        scores, results = run_rgb_complete_negative_rejection(
            dataset="en_refine",
            modelname="Groq",
            temperature=Temperature,
            passage_num=5,
            num_samples=300
        )

        # Export complete analysis
        df_main, df_summary = export_complete_analysis_csv(results, scores)

        print(f"\n✅ COMPLETE ANALYSIS FINISHED!")
        print(f"📊 Files created for analysis:")
        print(f"   1. rgb_complete_rejection_analysis.csv - All samples with details")
        print(f"   2. rgb_rejection_summary.csv - Summary statistics")
        print(f"\n📈 Key Results:")
        print(f"   Rej Rate: {scores['rej_rate']:.2f}%")
        print(f"   Rej* Rate: {scores['rej_star_rate']:.2f}%")
        print(f"   Agreement: {scores['agreement_rate']:.2f}%")

        return scores, df_main

    except Exception as e:
        print(f"❌ Error in complete analysis: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# ======================== RUN WITH YOUR MODELS ========================
if __name__ == "__main__":
    # Uses your GENERATION_MODEL and JUDGMENT_MODEL from the top of your code
    scores, df = run_complete_rgb_analysis()

"""**Information Integration**"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install groq

import json
import os
import time
import pandas as pd
from groq import Groq
from tqdm import tqdm
import random

# =============================================================================
# CONFIGURATION - Easy to modify
# =============================================================================
GROQ_API_KEY =  userdata.get('GROQ_API_KEY')  # Replace with your actual API key
TEMPERATURE = 0.2  # Easy to change - 0.0 (deterministic) to 1.0 (creative)
NOISE_RATIO = 0.4  # Proportion of negative documents (0.0 = no noise)
MAX_SAMPLES = None  # Set to number for testing, None for full dataset
# =============================================================================

# Set up Groq client
client = Groq(api_key=GROQ_API_KEY)

# Models to test
models = [
    "llama-3.3-70b-versatile",
    "llama-3.1-8b-instant",
    "gemma2-9b-it",
    "qwen/qwen3-32b",
    "deepseek-r1-distill-llama-70b"
]

# RGB System Instruction
system_instruction = """You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Please note that external documents may contain noisy or factually incorrect information. If the information in the document contains the correct answer, you will give an accurate answer. If the information in the document does not contain the answer, you will generate 'I can not answer the question because of the insufficient information in documents.' If there are inconsistencies with the facts in some of the documents, please generate the response 'There are factual errors in the provided documents.' and provide the correct answer."""

def load_dataset():
    """Load the en_int.json dataset"""
    try:
        return df_int
    except FileNotFoundError:
        print("en_int.json not found. Please upload it to Colab.")
        return []

def prepare_documents(positive_docs, negative_docs, noise_ratio=0.0):
    """
    Prepare 5 documents as per RGB methodology
    noise_ratio: proportion of negative documents (0.0 = no noise)
    """
    all_positive = []
    for doc_group in positive_docs:
        if isinstance(doc_group, list):
            all_positive.extend(doc_group)
        else:
            all_positive.append(doc_group)

    all_negative = []
    for doc in negative_docs:
        all_negative.append(doc)

    # Select 5 documents total
    num_negative = int(5 * noise_ratio)
    num_positive = 5 - num_negative

    # Randomly sample documents
    selected_positive = random.sample(all_positive, min(num_positive, len(all_positive)))
    selected_negative = random.sample(all_negative, min(num_negative, len(all_negative)))

    # Fill remaining slots if needed
    while len(selected_positive) + len(selected_negative) < 5:
        if len(all_positive) > len(selected_positive):
            selected_positive.append(random.choice(all_positive))
        elif len(all_negative) > len(selected_negative):
            selected_negative.append(random.choice(all_negative))
        else:
            break

    # Combine and shuffle
    all_docs = selected_positive + selected_negative
    random.shuffle(all_docs)

    return all_docs[:5]  # Ensure exactly 5 documents

def format_user_input(query, documents):
    """Format the user input following RGB format"""
    formatted_docs = "\n\n".join([f"Document {i+1}:\n{doc}" for i, doc in enumerate(documents)])
    return f"Document:\n{formatted_docs}\n\nQuestion:\n{query}"

def evaluate_information_integration(response, expected_answers):
    """
    Evaluate Information Integration response
    expected_answers format: [answer_list1, answer_list2, ...]
    """
    response_lower = response.lower()

    # Check if response contains rejection
    rejection_phrases = [
        "insufficient information",
        "cannot answer",
        "can not answer",
        "not enough information",
        "unable to answer"
    ]

    if any(phrase in response_lower for phrase in rejection_phrases):
        return {
            "evaluation": "Rejection",
            "score": 0.0,
            "components_found": 0,
            "total_components": len(expected_answers),
            "details": "Model rejected to answer"
        }

    # Check each answer component
    components_found = 0
    found_details = []

    for i, answer_group in enumerate(expected_answers):
        component_found = False
        found_answer = None

        for answer_variant in answer_group:
            if answer_variant.lower() in response_lower:
                component_found = True
                found_answer = answer_variant
                break

        if component_found:
            components_found += 1
            found_details.append(f"Component {i+1}: Found '{found_answer}'")
        else:
            found_details.append(f"Component {i+1}: Not found")

    # Calculate score
    total_components = len(expected_answers)
    score = components_found / total_components

    # Determine evaluation
    if components_found == total_components:
        evaluation = "Full Success"
    elif components_found > 0:
        evaluation = "Partial Success"
    else:
        evaluation = "Failure"

    return {
        "evaluation": evaluation,
        "score": score,
        "components_found": components_found,
        "total_components": total_components,
        "details": "; ".join(found_details)
    }

def test_single_model_instance(model_name, query, documents, expected_answers, test_id):
    """Test a single model on one instance"""
    try:
        user_input = format_user_input(query, documents)

        chat_completion = client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": user_input}
            ],
            model=model_name,
            temperature=TEMPERATURE,  # Uses global temperature setting
            max_tokens=500
        )

        response = chat_completion.choices[0].message.content
        evaluation_result = evaluate_information_integration(response, expected_answers)

        return {
            "test_id": test_id,
            "model": model_name,
            "temperature": TEMPERATURE,
            "query": query,
            "response": response,
            "expected_answers": expected_answers,
            "evaluation": evaluation_result["evaluation"],
            "score": evaluation_result["score"],
            "components_found": evaluation_result["components_found"],
            "total_components": evaluation_result["total_components"],
            "details": evaluation_result["details"],
            "documents_used": documents,
            "error": None
        }

    except Exception as e:
        return {
            "test_id": test_id,
            "model": model_name,
            "temperature": TEMPERATURE,
            "query": query,
            "response": None,
            "expected_answers": expected_answers,
            "evaluation": "Error",
            "score": 0.0,
            "components_found": 0,
            "total_components": len(expected_answers),
            "details": f"Error: {str(e)}",
            "documents_used": documents,
            "error": str(e)
        }

def run_information_integration_test(dataset, models, noise_ratio=0.0, max_samples=None):
    """Run the complete Information Integration test"""
    results = []

    # Limit samples if specified
    test_data = dataset[:max_samples] if max_samples else dataset

    print(f"Running Information Integration test on {len(test_data)} samples with {len(models)} models")
    print(f"Temperature: {TEMPERATURE}")
    print(f"Noise ratio: {noise_ratio}")
    print(f"{'='*60}")

    for i, (index, item) in enumerate(tqdm(test_data.iterrows(), desc="Processing test cases")):
        test_id = item.get('id', i)
        query = item['query']
        expected_answers = item['answer']

        # Prepare documents
        documents = prepare_documents(
            item['positive'],
            item['negative'],
            noise_ratio=noise_ratio
        )

        print(f"\nTest ID: {test_id}")
        print(f"Query: {query}")
        print(f"Expected components: {len(expected_answers)}")

        # Test each model
        for model_name in models:
            print(f"  Testing {model_name}...", end=" ")

            result = test_single_model_instance(
                model_name, query, documents, expected_answers, test_id
            )
            results.append(result)

            print(f"[{result['evaluation']} - Score: {result['score']:.2f}]")

            # Rate limiting
            time.sleep(0.5)

    return results

def analyze_results(results):
    """Analyze and summarize results"""
    df = pd.DataFrame(results)

    print(f"\n{'='*60}")
    print("INFORMATION INTEGRATION TEST ANALYSIS")
    print(f"{'='*60}")

    # Overall statistics
    print(f"Total tests run: {len(results)}")
    print(f"Models tested: {df['model'].nunique()}")
    print(f"Test cases: {df['test_id'].nunique()}")
    print(f"Temperature used: {TEMPERATURE}")

    # Results by model
    print(f"\n{'Model Performance:'}")
    print(f"{'-'*60}")

    model_stats = df.groupby('model').agg({
        'score': ['mean', 'std', 'count'],
        'evaluation': lambda x: (x == 'Full Success').sum(),
        'components_found': 'mean'
    }).round(3)

    model_stats.columns = ['Avg_Score', 'Std_Score', 'Total_Tests', 'Full_Success_Count', 'Avg_Components_Found']
    model_stats['Full_Success_Rate'] = (model_stats['Full_Success_Count'] / model_stats['Total_Tests'] * 100).round(2)

    print(model_stats)

    # Evaluation distribution
    print(f"\n{'Evaluation Distribution:'}")
    eval_dist = df['evaluation'].value_counts()
    eval_percent = (df['evaluation'].value_counts(normalize=True) * 100).round(2)
    for eval_type in eval_dist.index:
        print(f"  {eval_type}: {eval_dist[eval_type]} ({eval_percent[eval_type]}%)")

    # Error analysis
    error_count = df[df['evaluation'] == 'Error'].shape[0]
    if error_count > 0:
        print(f"\nErrors encountered: {error_count}")
        print("Error details:")
        error_details = df[df['evaluation'] == 'Error']['details'].value_counts()
        print(error_details)

    return df, model_stats

def save_results(results, model_stats, filename_prefix="information_integration"):
    """Save results to files"""
    # Include temperature in filename
    temp_suffix = f"_temp_{TEMPERATURE}"

    # Detailed results
    detailed_filename = f"{filename_prefix}{temp_suffix}_detailed_results.json"
    with open(detailed_filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    # CSV for easy analysis
    df = pd.DataFrame(results)
    csv_filename = f"{filename_prefix}{temp_suffix}_results.csv"
    df.to_csv(csv_filename, index=False)

    # Summary statistics
    summary_filename = f"{filename_prefix}{temp_suffix}_summary.json"
    summary_data = {
        "test_configuration": {
            "temperature": TEMPERATURE,
            "noise_ratio": NOISE_RATIO,
            "max_samples": MAX_SAMPLES
        },
        "model_statistics": model_stats.to_dict(),
        "overall_stats": {
            "total_tests": len(results),
            "average_score": df['score'].mean(),
            "models_tested": df['model'].nunique(),
            "test_cases": df['test_id'].nunique()
        }
    }

    with open(summary_filename, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)

    print(f"\nResults saved:")
    print(f"  Detailed results: {detailed_filename}")
    print(f"  CSV format: {csv_filename}")
    print(f"  Summary: {summary_filename}")

# Main execution
if __name__ == "__main__":
    # Display current configuration
    print(f"{'='*60}")
    print("CONFIGURATION")
    print(f"{'='*60}")
    print(f"Temperature: {TEMPERATURE}")
    print(f"Noise Ratio: {NOISE_RATIO}")
    print(f"Max Samples: {MAX_SAMPLES}")
    print(f"Models: {len(models)}")

    # Load dataset
    dataset = load_dataset()

    if dataset.empty:
        print("No dataset loaded. Please check the file path.")
    else:
        # Set random seed for reproducibility
        random.seed(42)

        # Run test
        results = run_information_integration_test(
            dataset=dataset,
            models=models,
            noise_ratio=NOISE_RATIO,
            max_samples=MAX_SAMPLES
        )

        # Analyze results
        df, model_stats = analyze_results(results)

        # Save results
        save_results(results, model_stats)

        print(f"\n{'='*60}")
        print("Test completed successfully!")
        print(f"Temperature used: {TEMPERATURE}")
        print("Check the saved files for detailed analysis.")

"""**FACT CHECK**"""

def run_rgb_factcheck_evaluation(
    dataset="en_fact",
    modelname="Groq",
    plm="deepseek-r1-distill-llama-70b",
    temperature=0.2,
    passage_num=5,
    noise_rate=0.2,
    correct_rate=0.0
):
    """
    RGB Fact-Checking Implementation
    Based on RGB paper methodology for counterfactual robustness
    """
    print("🔍 RGB FACT-CHECKING EVALUATION")
    print(f"Model: {modelname} ({plm})")
    print(f"Dataset: {dataset}")
    print(f"Temperature: {temperature}")
    print("-" * 60)

    # Step 1: Generate predictions if not exists
    print("Step 1: Generating fact-check predictions...")
    run_rgb_eval_new(
        modelname=modelname,
        dataset=dataset,
        plm=plm,
        temperature=temperature,
        noise_rate=noise_rate,
        correct_rate=correct_rate,
        passage_num=passage_num,
        factchecking=True
    )

    # Step 2: Analyze results using RGB methodology
    print("\nStep 2: Analyzing fact-check results...")

    resultpath = f"result-{dataset[:2]}/fact"
    prediction_file = f"/content/{resultpath}/prediction_{dataset}_{modelname}_temp{temperature}_noise{noise_rate}_passage{passage_num}_correct{correct_rate}.json"

    if not os.path.exists(prediction_file):
        raise FileNotFoundError(f"Prediction file not found: {prediction_file}")

    # Load results
    results = []
    with open(prediction_file, 'r') as f:
        for line in f:
            results.append(json.loads(line))

    # RGB Methodology: Calculate metrics
    total = len(results)
    error_detections = 0
    correct_after_detection = 0
    correct_answers = 0

    for result in results:
        prediction = result['prediction']
        label = result['label']
        factlabel = result['factlabel']

        # Error Detection (ED): Model detected factual errors
        if factlabel == 1:
            error_detections += 1

            # Error Correction (CR): Correct answer after detecting error
            if 0 not in label and 1 in label:
                correct_after_detection += 1

        # Overall accuracy with counterfactual documents
        if 0 not in label and 1 in label:
            correct_answers += 1

    # Calculate final metrics
    accuracy_with_docs = (correct_answers / total * 100) if total > 0 else 0
    error_detection_rate = (error_detections / total * 100) if total > 0 else 0
    error_correction_rate = (correct_after_detection / error_detections * 100) if error_detections > 0 else 0

    # Compile results
    factcheck_scores = {
        "model": modelname,
        "plm": plm,
        "dataset": dataset,
        "temperature": temperature,
        "total_instances": total,
        "accuracy_with_counterfactual_docs": accuracy_with_docs,
        "error_detections": error_detections,
        "error_detection_rate": error_detection_rate,
        "correct_after_detection": correct_after_detection,
        "error_correction_rate": error_correction_rate
    }

    # Save results
    result_file = prediction_file.replace(".json", "_factcheck_analysis.json")
    json.dump(factcheck_scores, open(result_file, 'w'), indent=4, ensure_ascii=False)

    # Display results
    print(f"\n📊 FACT-CHECKING RESULTS:")
    print("-" * 40)
    print(f"Total Instances: {total}")
    print(f"Accuracy with Docs: {accuracy_with_docs:.1f}%")
    print(f"Error Detection Rate: {error_detection_rate:.1f}%")
    print(f"Error Correction Rate: {error_correction_rate:.1f}%")
    print("-" * 40)

    # Show examples
    print(f"\n📝 SAMPLE OUTPUTS:")
    print("-" * 60)

    examples_shown = 0
    for result in results:
        if examples_shown >= 3:
            break

        detected = "✅ DETECTED" if result['factlabel'] == 1 else "❌ MISSED"
        correct = "✅ CORRECT" if (0 not in result['label'] and 1 in result['label']) else "❌ WRONG"

        print(f"\nExample {examples_shown + 1}: [{detected}] [{correct}]")
        print(f"Q: {result['query'][:70]}...")
        print(f"A: {result['prediction'][:100]}...")
        examples_shown += 1

    return factcheck_scores, results

def export_factcheck_to_csv(results, scores, output_file="factcheck_results.csv"):
    """Export fact-checking results to CSV"""
    import pandas as pd

    # Prepare detailed data
    csv_data = []
    for result in results:
        csv_data.append({
            "id": result["id"],
            "query": result["query"],
            "ground_truth": result["ans"],
            "prediction": result["prediction"],
            "error_detected": result["factlabel"] == 1,
            "detection_phrase_found": "factual errors" in result["prediction"].lower(),
            "answer_correct": (0 not in result["label"] and 1 in result["label"]),
            "noise_rate": result.get("noise_rate", 0),
            "model": scores["model"],
            "temperature": scores["temperature"]
        })

    # Create DataFrame
    df = pd.DataFrame(csv_data)

    # Save to CSV
    output_path = f'/content/{output_file}'
    df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"📁 Detailed results exported to: {output_path}")

    # Create summary CSV
    summary_data = {
        "metric": [
            "Total Instances",
            "Accuracy with Docs (%)",
            "Error Detection Rate (%)",
            "Error Correction Rate (%)",
            "Model",
            "Temperature"
        ],
        "value": [
            scores["total_instances"],
            f"{scores['accuracy_with_counterfactual_docs']:.1f}",
            f"{scores['error_detection_rate']:.1f}",
            f"{scores['error_correction_rate']:.1f}",
            scores["model"],
            scores["temperature"]
        ]
    }

    summary_df = pd.DataFrame(summary_data)
    summary_file = output_file.replace('.csv', '_summary.csv')
    summary_path = f'/content/{summary_file}'
    summary_df.to_csv(summary_path, index=False, encoding='utf-8')
    print(f"📁 Summary exported to: {summary_path}")

    return df, summary_df

def run_factcheck_workflow():
    """Complete fact-checking workflow"""
    print("🚀 RGB FACT-CHECKING WORKFLOW")
    print("=" * 50)

    try:
        # Run evaluation
        scores, results = run_rgb_factcheck_evaluation(
            dataset="en_fact",
            modelname="Groq",
            plm=GENERATION_MODEL,
            temperature=0.2,
            passage_num=5,
            noise_rate=0.2,
            correct_rate=0.0
        )

        # Export results
        df, summary_df = export_factcheck_to_csv(
            results,
            scores,
            output_file=f"factcheck_{scores['model']}_{scores['plm'].replace('-', '_')}_results.csv"
        )

        print(f"\n✅ Fact-checking evaluation completed successfully!")
        print(f"📈 Error Detection: {scores['error_detection_rate']:.1f}%")
        print(f"📈 Error Correction: {scores['error_correction_rate']:.1f}%")

        return scores, df

    except Exception as e:
        print(f"❌ Error in fact-checking workflow: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def analyze_factcheck_patterns(results):
    """Analyze patterns in fact-checking responses"""
    print("\n🔍 FACT-CHECKING PATTERN ANALYSIS:")
    print("-" * 50)

    # Analyze detection phrases
    detection_phrases = [
        "factual errors", "factual error", "incorrect information",
        "wrong information", "inaccurate", "error in the document"
    ]

    phrase_counts = {phrase: 0 for phrase in detection_phrases}

    for result in results:
        prediction = result['prediction'].lower()
        for phrase in detection_phrases:
            if phrase in prediction:
                phrase_counts[phrase] += 1

    print("Detection Phrase Usage:")
    for phrase, count in phrase_counts.items():
        if count > 0:
            print(f"  '{phrase}': {count} times")

    # Analyze detection vs correction
    detected_count = sum(1 for r in results if r['factlabel'] == 1)
    corrected_count = sum(1 for r in results if r['factlabel'] == 1 and (0 not in r['label'] and 1 in r['label']))

    print(f"\nDetection vs Correction:")
    print(f"  Errors Detected: {detected_count}")
    print(f"  Correctly Answered After Detection: {corrected_count}")

    if detected_count > 0:
        correction_efficiency = (corrected_count / detected_count) * 100
        print(f"  Correction Efficiency: {correction_efficiency:.1f}%")

# ======================== USAGE ========================
def run_complete_factcheck():
    """Run complete fact-checking evaluation"""

    # Main evaluation
    scores, df = run_factcheck_workflow()

    if scores and df is not None:
        # Additional analysis
        with open(f"/content/result-en/fact/prediction_en_fact_Groq_temp0.2_noise0.0_passage5_correct0.0.json", 'r') as f:
            results = [json.loads(line) for line in f]

        analyze_factcheck_patterns(results)

    return scores

# ======================== RUN ========================
if __name__ == "__main__":
    scores = run_complete_factcheck()

